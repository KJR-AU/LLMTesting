{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple LLM / RAG chat model with chat history saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model and embedding\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up basic chroma vectorstore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import document_handler\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "\n",
    "chroma_collection_name = \"LangChainCollection\"\n",
    "\n",
    "import chromadb\n",
    "new_client = chromadb.EphemeralClient()\n",
    "\n",
    "vectorstore_initialize = Chroma.from_documents(\n",
    "    document_handler.processed_texts,\n",
    "    embedding=embedding,\n",
    "    collection_name=chroma_collection_name,\n",
    "    client=new_client,\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=new_client,\n",
    "    collection_name=chroma_collection_name,\n",
    "    embedding_function=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolates are often used in confectionery and desserts, and they can be found in a wide range of products, including bars, truffles, candies, and baked goods. Chocolate is enjoyed worldwide and is often associated with indulgence and celebration. Additionally, cocoa has been linked to various potential health benefits, such as antioxidants and mood enhancement, when consumed in moderation.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolate comes in various forms, such as dark chocolate, milk chocolate, and white chocolate. Dark chocolate contains a higher percentage of cocoa solids and less sugar, giving it a more intense and bitter flavor. Milk chocolate, on the other hand, includes milk solids in addition to cocoa, creating a sweeter and creamier taste. White chocolate consists mainly of cocoa butter, sugar, and milk solids but lacks cocoa solids.', metadata={'source': 'test_data\\\\Chocolate.txt'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolates are often used in confectionery and desserts, and they can be found in a wide range of products, including bars, truffles, candies, and baked goods. Chocolate is enjoyed worldwide and is often associated with indulgence and celebration. Additionally, cocoa has been linked to various potential health benefits, such as antioxidants and mood enhancement, when consumed in moderation.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolate comes in various forms, such as dark chocolate, milk chocolate, and white chocolate. Dark chocolate contains a higher percentage of cocoa solids and less sugar, giving it a more intense and bitter flavor. Milk chocolate, on the other hand, includes milk solids in addition to cocoa, creating a sweeter and creamier taste. White chocolate consists mainly of cocoa butter, sugar, and milk solids but lacks cocoa solids.', metadata={'source': 'test_data\\\\Chocolate.txt'})]\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore_initialize.similarity_search(\"What is Chocolate?\")\n",
    "print(docs)\n",
    "docs = vectorstore.similarity_search(\"What is Chocolate?\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.agents import tool\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"output\", input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Chat History: {chat_history}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "\n",
    "system_message_template = (\n",
    "    \"You are a helpful assistant who helps answer questions. Answer only the facts based on the context. \"\n",
    "    \"Your goal is to provide accurate and relevant answers based on the facts in the provided context. \"\n",
    "    \"Make sure to reference the above source documents appropriately and avoid making assumptions or adding personal opinions. \"\n",
    "    \"Emphasize the use of facts from the provided source documents. \"\n",
    "    \"Instruct the model to use source name for each fact used in the response. \"\n",
    "    \"Avoid generating speculative or generalized information. \"\n",
    "    \"Use square brackets to reference the source, e.g. [info1.txt]. \"\n",
    "    \"Do not combine sources, list each source separately, e.g. [info1.txt][info2.pdf].\\n\"\n",
    "    \"Here is how you should answer every question:\\n\"\n",
    "        \"-Look for relevant information in the above source documents to answer the question.\\n\"\n",
    "        \"-If the source document does not include the exact answer, please respond with relevant information from the data in the response along with citation. You must include a citation to each document referenced.\\n\"\n",
    "        \"-If you cannot find answer in below sources, respond with I am not sure. Do not provide personal opinions or assumptions and do not include citations.\\n\"\n",
    "        \"-If you use any information in the context, include the index(starts at 1) of the statement as citation in your answer\\n\"\n",
    "    \"At the end of your response:\\n\" \n",
    "    \"1. Add key words from the paragraphs. \\n\"\n",
    "    \"2. Suggest a further question that can be answered by the paragraphs provided. \\n\"\n",
    "    \"3. Create a source list of source name, author name, and a link for each document you cited.\\n\"\n",
    "    \"{context}\"\n",
    "\n",
    ")\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message_template),\n",
    "        # MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"human\", \"{question}\"),\n",
    "        # MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set llm chain\n",
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "}\n",
    "\n",
    "chain = (\n",
    "    loaded_memory\n",
    "    | standalone_question\n",
    "    | {\"context\": itemgetter(\"standalone_question\") | retriever | format_docs, \"question\": lambda x: x[\"standalone_question\"]}\n",
    "    | final_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Question': 'What is Chocolate?', 'Follow up': 'No'}, {'Question': 'Is that a word?', 'Follow up': 'Yes'}, {'Question': 'Write the following words and then continue: g\\\\thumb|350', 'Follow up': 'No'}, {'Question': 'What is iPhone?', 'Follow up': 'No'}, {'Question': 'What is Task Decomposition?', 'Follow up': ' No'}]\n",
      "['What is Chocolate?', 'Is that a word?', 'Write the following words and then continue: g\\\\thumb|350', 'What is iPhone?', 'What is Task Decomposition?']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./test_data/Questions.csv', delimiter=',')\n",
    "tuples = [tuple(x) for x in df.values]\n",
    "dicts = df.to_dict('records')\n",
    "\n",
    "print(dicts)\n",
    "\n",
    "questions = list(map(lambda x : x['Question'], dicts))\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dict in dicts:\n",
    "#         question = dict['Question']\n",
    "#         follow_up = dict['Follow up']\n",
    "#         if follow_up == \"No\":\n",
    "#                 memory.clear()\n",
    "#         print(memory.load_memory_variables({}))\n",
    "#         llm_response = chain.invoke({\"question\": question})\n",
    "#         print(llm_response)\n",
    "#         memory.save_context({\"question\":question}, {\"output\":llm_response})\n",
    "\n",
    "# memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import TruChain, Feedback, Tru\n",
    "from trulens_eval.schema import FeedbackResult\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.middle[1].steps.context.middle[0].get_relevant_documents.rets.collect() .\n",
      "âœ… In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In qs_relevance, input statement will be set to __record__.app.middle[1].steps.context.middle[0].get_relevant_documents.rets .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI(model_engine=\"gpt-3.5-turbo\")\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(chain)\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "grounded = Groundedness(groundedness_provider=OpenAI(model_engine=\"gpt-3.5-turbo\"))\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.qs_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Task Decomposition?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is simple one call\n",
    "with tru_recorder as recording:\n",
    "    llm_response = chain.invoke({\"question\":\"What is Task Decomposition?\"})\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The provided source documents do not contain any information about the term \"Multiple Question.\" I am not sure what you are referring to.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'m sorry, but I couldn\\'t find any information about the definition of \"Multiple Question\" in the provided paragraphs.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure about the term \"Multiple Question\" as it is not mentioned in the provided sources.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure what you mean by \"Multiple Question.\" Could you please provide more information or clarify your question?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Multiple Question?'}].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am sorry, but I could not find any information about the definition of \"Multiple Question\" in the provided source documents.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tru_recorder_stress_one_question = TruChain(chain,\n",
    "    app_id='Chain1_One_Q_Multiple',\n",
    "    feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])\n",
    "\n",
    "# This is simple multiple call on same query\n",
    "with tru_recorder_stress_one_question as recording:\n",
    "    for i in range(10):\n",
    "        llm_response = chain.invoke({\"question\": \"What is Multiple Question?\"})\n",
    "        display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Chocolate?'}].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter. [1.txt]\n",
      "{'history': [HumanMessage(content='What is Chocolate?'), AIMessage(content='Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter. [1.txt]')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'Is that a word?'}].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, \"Chocolate\" is a word. [info1.txt]\n",
      "{'history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'Write the following words and then continue: g\\\\thumb|350'}].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are the words you requested: chocolate, cocoa beans, cacao tree, cocoa mass, cocoa solids, cocoa butter, dark chocolate, milk chocolate, white chocolate, confectionery, desserts, bars, truffles, candies, baked goods, indulgence, celebration, antioxidants, mood enhancement.\n",
      "\n",
      "Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree [info1.txt]. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass [info1.txt]. This cocoa mass is then further processed to extract cocoa solids and cocoa butter [info1.txt].\n",
      "\n",
      "Chocolate comes in various forms, such as dark chocolate, milk chocolate, and white chocolate [info1.txt]. Dark chocolate contains a higher percentage of cocoa solids and less sugar, giving it a more intense and bitter flavor [info1.txt]. Milk chocolate includes milk solids in addition to cocoa, creating a sweeter and creamier taste [info1.txt]. White chocolate consists mainly of cocoa butter, sugar, and milk solids but lacks cocoa solids [info1.txt].\n",
      "\n",
      "Chocolates are often used in confectionery and desserts, and they can be found in a wide range of products, including bars, truffles, candies, and baked goods [info1.txt]. Chocolate is enjoyed worldwide and is often associated with indulgence and celebration [info1.txt]. Additionally, cocoa has been linked to various potential health benefits, such as antioxidants and mood enhancement, when consumed in moderation [info1.txt].\n",
      "\n",
      "Key words: chocolate, cocoa beans, cacao tree, cocoa mass, cocoa solids, cocoa butter, dark chocolate, milk chocolate, white chocolate, confectionery, desserts, bars, truffles, candies, baked goods, indulgence, celebration, antioxidants, mood enhancement.\n",
      "\n",
      "Suggested further question: What are some potential health benefits of consuming cocoa? \n",
      "\n",
      "Source list:\n",
      "- [info1.txt]: Cocoa and Chocolate, Author: Unknown, Link: Unknown\n",
      "{'history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is iPhone?'}].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not sure.\n",
      "{'history': [HumanMessage(content='What is iPhone?'), AIMessage(content='I am not sure.')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Unsure what the main input string is for the call to invoke with args [{'question': 'What is Task Decomposition?'}].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not sure.\n"
     ]
    }
   ],
   "source": [
    "tru_recorder_more_questions = TruChain(chain,\n",
    "    app_id='Chain1_More_Qs',\n",
    "    feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])\n",
    "\n",
    "# This is simple multiple call on same query\n",
    "with tru_recorder_more_questions as recording:\n",
    "    for dict in dicts:\n",
    "        question = dict['Question']\n",
    "        follow_up = dict['Follow up']\n",
    "        if follow_up == \"No\":\n",
    "                memory.clear()\n",
    "        print(memory.load_memory_variables({}))\n",
    "        llm_response = chain.invoke({\"question\": question})\n",
    "        print(llm_response)\n",
    "        memory.save_context({\"question\":question}, {\"output\":llm_response})\n",
    "\n",
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Recording context recorded more than 1 record. You can get them with ctx.records, ctx[i], or `for r in ctx: ...`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# The record of the app invocation can be retrieved from the `recording`:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m rec \u001b[38;5;241m=\u001b[39m \u001b[43mrecording\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# use .get if only one record\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# recs = recording.records # use .records if multiple\u001b[39;00m\n\u001b[0;32m      6\u001b[0m display(rec)\n",
      "File \u001b[1;32mc:\\Users\\jiheu\\LLMTesting\\LLMTesting\\.venv\\Lib\\site-packages\\trulens_eval\\app.py:347\u001b[0m, in \u001b[0;36mRecordingContext.get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording context did not record any records.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecords) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording context recorded more than 1 record. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can get them with ctx.records, ctx[i], or `for r in ctx: ...`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m     )\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecords[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Recording context recorded more than 1 record. You can get them with ctx.records, ctx[i], or `for r in ctx: ...`."
     ]
    }
   ],
   "source": [
    "# The record of the app invocation can be retrieved from the `recording`:\n",
    "\n",
    "rec = recording.get() # use .get if only one record\n",
    "# recs = recording.records # use .records if multiple\n",
    "\n",
    "display(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results of the feedback functions can be rertireved from the record. These\n",
    "# are `Future` instances (see `concurrent.futures`). You can use `as_completed`\n",
    "# to wait until they have finished evaluating.\n",
    "\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "for feedback_future in  as_completed(rec.feedback_results):\n",
    "    feedback, feedback_result = feedback_future.result()\n",
    "    \n",
    "    feedback: Feedback\n",
    "    feedbac_result: FeedbackResult\n",
    "\n",
    "    display(feedback.name, feedback_result.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[\"Chain1_ChatApplication\"])\n",
    "\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[\"Chain1_ChatApplication\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore\n",
    "\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
