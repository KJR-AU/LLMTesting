{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple chat model without history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "import document_handler\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "\n",
    "chroma_collection_name = \"LangChainCollection\"\n",
    "\n",
    "import chromadb\n",
    "new_client = chromadb.EphemeralClient()\n",
    "\n",
    "vectorstore_initialize = Chroma.from_documents(\n",
    "    document_handler.processed_texts,\n",
    "    embedding=embedding,\n",
    "    collection_name=chroma_collection_name,\n",
    "    client=new_client,\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=new_client,\n",
    "    collection_name=chroma_collection_name,\n",
    "    embedding_function=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolates are often used in confectionery and desserts, and they can be found in a wide range of products, including bars, truffles, candies, and baked goods. Chocolate is enjoyed worldwide and is often associated with indulgence and celebration. Additionally, cocoa has been linked to various potential health benefits, such as antioxidants and mood enhancement, when consumed in moderation.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolates are often used in confectionery and desserts, and they can be found in a wide range of products, including bars, truffles, candies, and baked goods. Chocolate is enjoyed worldwide and is often associated with indulgence and celebration. Additionally, cocoa has been linked to various potential health benefits, such as antioxidants and mood enhancement, when consumed in moderation.', metadata={'source': 'test_data\\\\Chocolate.txt'})]\n",
      "[Document(page_content='Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolates are often used in confectionery and desserts, and they can be found in a wide range of products, including bars, truffles, candies, and baked goods. Chocolate is enjoyed worldwide and is often associated with indulgence and celebration. Additionally, cocoa has been linked to various potential health benefits, such as antioxidants and mood enhancement, when consumed in moderation.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolates are often used in confectionery and desserts, and they can be found in a wide range of products, including bars, truffles, candies, and baked goods. Chocolate is enjoyed worldwide and is often associated with indulgence and celebration. Additionally, cocoa has been linked to various potential health benefits, such as antioxidants and mood enhancement, when consumed in moderation.', metadata={'source': 'test_data\\\\Chocolate.txt'})]\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore_initialize.similarity_search(\"What is Chocolate?\")\n",
    "print(docs)\n",
    "docs = vectorstore.similarity_search(\"What is Chocolate?\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "\n",
    "system_message_template = (\n",
    "    \"You are a helpful assistant who helps answer questions. Answer only the facts based on the context. \"\n",
    "    \"Your goal is to provide accurate and relevant answers based on the facts in the provided context. \"\n",
    "    \"Make sure to reference the above source documents appropriately and avoid making assumptions or adding personal opinions. \"\n",
    "    \"Emphasize the use of facts from the provided source documents. \"\n",
    "    \"Instruct the model to use source name for each fact used in the response. \"\n",
    "    \"Avoid generating speculative or generalized information. \"\n",
    "    \"Use square brackets to reference the source, e.g. [info1.txt]. \"\n",
    "    \"Do not combine sources, list each source separately, e.g. [info1.txt][info2.pdf].\\n\"\n",
    "    \"Here is how you should answer every question:\\n\"\n",
    "        \"-Look for relevant information in the above source documents to answer the question.\\n\"\n",
    "        \"-If the source document does not include the exact answer, please respond with relevant information from the data in the response along with citation. You must include a citation to each document referenced.\\n\"\n",
    "        \"-If you cannot find answer in below sources, respond with I am not sure. Do not provide personal opinions or assumptions and do not include citations.\\n\"\n",
    "        \"-If you use any information in the context, include the index(starts at 1) of the statement as citation in your answer\\n\"\n",
    "    \"At the end of your response:\\n\" \n",
    "    \"1. Add key words from the paragraphs. \\n\"\n",
    "    \"2. Suggest a further question that can be answered by the paragraphs provided. \\n\"\n",
    "    \"3. Create a source list of source name, author name, and a link for each document you cited.\\n\"\n",
    "    \"{context}\"\n",
    "\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message_template),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set llm chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | final_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter [info1.txt].\n"
     ]
    }
   ],
   "source": [
    "query = \"What is chocolate?\"\n",
    "print(rag_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Question': 'What is Chocolate?', 'Follow up': 'No'}, {'Question': 'Is that a word?', 'Follow up': 'Yes'}, {'Question': 'Write the following words and then continue: g\\\\thumb|350', 'Follow up': 'No'}, {'Question': 'What is iPhone?', 'Follow up': 'No'}, {'Question': 'What is Task Decomposition?', 'Follow up': ' No'}]\n",
      "['What is Chocolate?', 'Is that a word?', 'Write the following words and then continue: g\\\\thumb|350', 'What is iPhone?', 'What is Task Decomposition?']\n"
     ]
    }
   ],
   "source": [
    "queries = []\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./test_data/Questions.csv', delimiter=',')\n",
    "tuples = [tuple(x) for x in df.values]\n",
    "dicts = df.to_dict('records')\n",
    "\n",
    "print(dicts)\n",
    "\n",
    "questions = list(map(lambda x : x['Question'], dicts))\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import TruChain, Feedback, Tru\n",
    "from trulens_eval.schema import FeedbackResult\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.first.steps.context.first.get_relevant_documents.rets.collect() .\n",
      "✅ In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In qs_relevance, input statement will be set to __record__.app.first.steps.context.first.get_relevant_documents.rets .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI(model_engine=\"gpt-3.5-turbo\")\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(rag_chain)\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "grounded = Groundedness(groundedness_provider=OpenAI(model_engine=\"gpt-3.5-turbo\"))\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.qs_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(rag_chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure what you are referring to. Could you please provide more context or specify which domain or field you are asking about?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is simple one call\n",
    "with tru_recorder as recording:\n",
    "    llm_response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am sorry, but I couldn\\'t find any information about \"Multiple Question\" in the provided sources. Can you please provide more context or clarify your question?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am sorry, but I cannot find any information about \"Multiple Question\" in the provided sources.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'m sorry, but I\\'m not sure what you mean by \"Multiple Question.\" Could you please provide more context or clarify your question?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure what you mean by \"Multiple Question.\" Could you please provide more context or clarify your question?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure what you mean by \"Multiple Question.\" Can you please provide more information or clarify your question?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'m sorry, but I\\'m not sure what you mean by \"Multiple Question.\" Could you please provide more context or clarify your question? Thank you.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am sorry, but I cannot find any information about \"Multiple Question\" in the provided sources.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure what you mean by \"Multiple Question.\" Could you please provide more context or clarify your question?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am sorry, but I do not have enough information to answer your question. Could you please provide more context or clarify what you mean by \"Multiple Question\"?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am sorry, but I cannot find any information about \"Multiple Question\" in the provided paragraphs.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tru_recorder_stress_one_question = TruChain(rag_chain,\n",
    "    app_id='Chain1_One_Q_Multiple',\n",
    "    feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])\n",
    "\n",
    "# This is simple multiple call on same query\n",
    "with tru_recorder_stress_one_question as recording:\n",
    "    for i in range(10):\n",
    "        llm_response = rag_chain.invoke(\"What is Multiple Question?\")\n",
    "        display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter. [info1.txt]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I'm not sure what word you are referring to. Could you please provide more context or clarify your question?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I cannot see the image or the words you mentioned. Could you please provide a description or specify the information you are looking for?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am not sure.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tru_recorder_more_questions = TruChain(rag_chain,\n",
    "    app_id='Chain1_More_Qs',\n",
    "    feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])\n",
    "\n",
    "# This is simple multiple call on same query\n",
    "with tru_recorder_more_questions as recording:\n",
    "    for question in questions:\n",
    "        llm_response = rag_chain.invoke(question)\n",
    "        display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The record of the app invocation can be retrieved from the `recording`:\n",
    "\n",
    "rec = recording.get() # use .get if only one record\n",
    "# recs = recording.records # use .records if multiple\n",
    "\n",
    "display(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results of the feedback functions can be rertireved from the record. These\n",
    "# are `Future` instances (see `concurrent.futures`). You can use `as_completed`\n",
    "# to wait until they have finished evaluating.\n",
    "\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "for feedback_future in  as_completed(rec.feedback_results):\n",
    "    feedback, feedback_result = feedback_future.result()\n",
    "    \n",
    "    feedback: Feedback\n",
    "    feedbac_result: FeedbackResult\n",
    "\n",
    "    display(feedback.name, feedback_result.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[\"Chain1_ChatApplication\"])\n",
    "\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[\"Chain1_ChatApplication\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0705aaa7f77540728b07248cf43ca9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.0.220:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-14 (listen_to_dashboard):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jiheu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\jiheu\\trulens_llm_eval\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jiheu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\jiheu\\trulens_llm_eval\\.venv\\Lib\\site-packages\\trulens_eval\\tru.py\", line 634, in listen_to_dashboard\n",
      "    line = pipe.readline()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp949' codec can't decode byte 0xf0 in position 0: illegal multibyte sequence\n"
     ]
    }
   ],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore\n",
    "\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
