{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple LLM / RAG chat model with chat history saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model and embedding\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up basic chroma vectorstore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import document_handler\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "\n",
    "chroma_collection_name = \"LangChainCollection\"\n",
    "\n",
    "import chromadb\n",
    "new_client = chromadb.EphemeralClient()\n",
    "\n",
    "vectorstore_initialize = Chroma.from_documents(\n",
    "    document_handler.processed_texts,\n",
    "    embedding=embedding,\n",
    "    collection_name=chroma_collection_name,\n",
    "    client=new_client,\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=new_client,\n",
    "    collection_name=chroma_collection_name,\n",
    "    embedding_function=embedding,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolates are often used in confectionery and desserts, and they can be found in a wide range of products, including bars, truffles, candies, and baked goods. Chocolate is enjoyed worldwide and is often associated with indulgence and celebration. Additionally, cocoa has been linked to various potential health benefits, such as antioxidants and mood enhancement, when consumed in moderation.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolate comes in various forms, such as dark chocolate, milk chocolate, and white chocolate. Dark chocolate contains a higher percentage of cocoa solids and less sugar, giving it a more intense and bitter flavor. Milk chocolate, on the other hand, includes milk solids in addition to cocoa, creating a sweeter and creamier taste. White chocolate consists mainly of cocoa butter, sugar, and milk solids but lacks cocoa solids.', metadata={'source': 'test_data\\\\Chocolate.txt'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Chocolate is a sweet, usually brown, food product made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is then further processed to extract cocoa solids and cocoa butter.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolates are often used in confectionery and desserts, and they can be found in a wide range of products, including bars, truffles, candies, and baked goods. Chocolate is enjoyed worldwide and is often associated with indulgence and celebration. Additionally, cocoa has been linked to various potential health benefits, such as antioxidants and mood enhancement, when consumed in moderation.', metadata={'source': 'test_data\\\\Chocolate.txt'}), Document(page_content='Chocolate comes in various forms, such as dark chocolate, milk chocolate, and white chocolate. Dark chocolate contains a higher percentage of cocoa solids and less sugar, giving it a more intense and bitter flavor. Milk chocolate, on the other hand, includes milk solids in addition to cocoa, creating a sweeter and creamier taste. White chocolate consists mainly of cocoa butter, sugar, and milk solids but lacks cocoa solids.', metadata={'source': 'test_data\\\\Chocolate.txt'})]\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore_initialize.similarity_search(\"What is Chocolate?\")\n",
    "print(docs)\n",
    "docs = vectorstore.similarity_search(\"What is Chocolate?\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.agents import tool\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"output\", input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Chat History: {chat_history}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "\n",
    "system_message_template = (\n",
    "    \"You are a helpful assistant who helps answer questions. Answer only the facts based on the context. \"\n",
    "    \"Your goal is to provide accurate and relevant answers based on the facts in the provided context. \"\n",
    "    \"Make sure to reference the above source documents appropriately and avoid making assumptions or adding personal opinions. \"\n",
    "    \"Emphasize the use of facts from the provided source documents. \"\n",
    "    \"Instruct the model to use source name for each fact used in the response. \"\n",
    "    \"Avoid generating speculative or generalized information. \"\n",
    "    \"Use square brackets to reference the source, e.g. [info1.txt]. \"\n",
    "    \"Do not combine sources, list each source separately, e.g. [info1.txt][info2.pdf].\\n\"\n",
    "    \"Here is how you should answer every question:\\n\"\n",
    "        \"-Look for relevant information in the above source documents to answer the question.\\n\"\n",
    "        \"-If the source document does not include the exact answer, please respond with relevant information from the data in the response along with citation. You must include a citation to each document referenced.\\n\"\n",
    "        \"-If you cannot find answer in below sources, respond with I am not sure. Do not provide personal opinions or assumptions and do not include citations.\\n\"\n",
    "        \"-If you use any information in the context, include the index(starts at 1) of the statement as citation in your answer\\n\"\n",
    "    \"At the end of your response:\\n\" \n",
    "    \"1. Add key words from the paragraphs. \\n\"\n",
    "    \"2. Suggest a further question that can be answered by the paragraphs provided. \\n\"\n",
    "    \"3. Create a source list of source name, author name, and a link for each document you cited.\\n\"\n",
    "    \"{context}\"\n",
    "\n",
    ")\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message_template),\n",
    "        # MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"human\", \"{question}\"),\n",
    "        # MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set llm chain\n",
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "}\n",
    "\n",
    "chain = (\n",
    "    loaded_memory\n",
    "    | standalone_question\n",
    "    | {\"context\": itemgetter(\"standalone_question\") | retriever | format_docs, \"question\": lambda x: x[\"standalone_question\"]}\n",
    "    | final_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Question': 'What is Chocolate?', 'Follow up': 'No'}, {'Question': 'Is that a word?', 'Follow up': 'Yes'}, {'Question': 'Write the following words and then continue: g\\\\thumb|350', 'Follow up': 'No'}, {'Question': 'What is iPhone?', 'Follow up': 'No'}, {'Question': 'What is Task Decomposition?', 'Follow up': ' No'}]\n",
      "['What is Chocolate?', 'Is that a word?', 'Write the following words and then continue: g\\\\thumb|350', 'What is iPhone?', 'What is Task Decomposition?']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./test_data/Questions.csv', delimiter=',')\n",
    "tuples = [tuple(x) for x in df.values]\n",
    "dicts = df.to_dict('records')\n",
    "\n",
    "print(dicts)\n",
    "\n",
    "questions = list(map(lambda x : x['Question'], dicts))\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chocolate is a sweet food product that is usually brown in color and is made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is further processed to extract cocoa solids and cocoa butter [info1.txt].\n",
      "{'history': [HumanMessage(content='What is Chocolate?'), AIMessage(content='Chocolate is a sweet food product that is usually brown in color and is made from cocoa beans, which are the seeds of the cacao tree. The process of making chocolate involves harvesting and fermenting cacao beans, drying them, and then roasting and grinding them to produce cocoa mass. This cocoa mass is further processed to extract cocoa solids and cocoa butter [info1.txt].')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, \"chocolate\" is a word. [source1.txt]\n",
      "{'history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot view or process images. Could you please provide the text description or context of the image so that I can assist you further?\n",
      "{'history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided source documents do not contain information about the meaning of \"iPhone.\" Therefore, I am unable to provide an answer based on the given sources.\n",
      "{'history': [HumanMessage(content='What is iPhone?'), AIMessage(content='The provided source documents do not contain information about the meaning of \"iPhone.\" Therefore, I am unable to provide an answer based on the given sources.')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided source documents do not contain information about the meaning of \"Task Decomposition.\" Therefore, I am unable to provide an answer with a specific citation.\n"
     ]
    }
   ],
   "source": [
    "for dict in dicts:\n",
    "        question = dict['Question']\n",
    "        follow_up = dict['Follow up']\n",
    "        answer = dict['Answer']\n",
    "        if follow_up == \"No\":\n",
    "                memory.clear()\n",
    "        print(memory.load_memory_variables({}))\n",
    "        llm_response = chain.invoke({\"question\": question})\n",
    "        print(llm_response)\n",
    "        memory.save_context({\"question\":question}, {\"output\":llm_response})\n",
    "\n",
    "memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = 'https://api.smith.langchain.com'\n",
    "\n",
    "import uuid\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "uid = uuid.uuid4()\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to run and evaluator\n",
    "https://docs.smith.langchain.com/tracing/use-cases/track-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "# from langsmith.schemas import Example, Run\n",
    "\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# class SentimentEvaluator(RunEvaluator):\n",
    "#     def __init__(self):\n",
    "#         prompt = \"\"\"Is the predominant sentiment in the following statement positive, negative, or neutral?\n",
    "# ---------\n",
    "# Statement: {input}\n",
    "# ---------\n",
    "# Respond in one word: positive, negative, or neutral.\n",
    "# Sentiment:\"\"\"\n",
    "\n",
    "#         llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "#         self.chain = LLMChain.from_string(llm=llm, template=prompt)\n",
    "\n",
    "#     def evaluate_run(self, run: Run, example: Example) -> EvaluationResult:\n",
    "#         input_str = str(list(run.inputs.values())[0])\n",
    "#         prediction = self.chain.run(input_str)\n",
    "#         # Strip the prompt\n",
    "#         prediction = prediction.strip()\n",
    "#         score = {\"positive\": 1, \"negative\": -1, \"neutral\": 0}.get(prediction)\n",
    "#         return EvaluationResult(\n",
    "#             key=\"sentiment\",\n",
    "#             value=prediction,\n",
    "#             score=score,\n",
    "#         )\n",
    "\n",
    "# evaluator = SentimentEvaluator()\n",
    "# for run in client.list_runs(\n",
    "#     project_name=\"my-project\",\n",
    "#     execution_order=1, # Do not return child / nested runs\n",
    "# ):\n",
    "#         client.evaluate_run(run, evaluator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for question in questions:\n",
    "#     result = chain.invoke(question)\n",
    "#     # print(result)\n",
    "\n",
    "#     question=result.get(\"input\", None)\n",
    "#     actual_output=result.get(\"output\", None)\n",
    "#     retrieval_context = [result.get(\"context\", None)]\n",
    "#     input = final_prompt.format(question=result[\"input\"], context=result[\"context\"])\n",
    "\n",
    "#     print(input)\n",
    "#     print(actual_output)\n",
    "#     print(retrieval_context)\n",
    "\n",
    "#     test_case = LLMTestCase(\n",
    "#         input=final_prompt.format(question=input, context=retrieval_context),\n",
    "#         actual_output=actual_output,\n",
    "#         retrieval_context = retrieval_context\n",
    "#     )\n",
    "\n",
    "#     # print(assert_test(test_case, metrics))\n",
    "#     test_cases.append(test_case)\n",
    "\n",
    "# print(evaluate(test_cases, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([e for e in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"Simple llm langsmith evaluation dataset - {uid}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"An example agent evals dataset\",\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": e} for e in questions],\n",
    "    # outputs=[e[\"outputs\"] for e in examples], # Outputs are optional, but recommended.\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'simple_llm_testing-3' at:\n",
      "https://smith.langchain.com/o/99caf4db-f19e-57e9-bc93-12af0e3dd026/datasets/290c08a2-d150-4a7b-a243-60db3c6f07b5/compare?selectedSessions=99ac6cef-e7f7-4538-9ed1-c115d966c565\n",
      "\n",
      "View all tests for Dataset Basic llm langsmith evaluation dataset - 82127b35-adf9-4d30-8893-c91b6579c87d at:\n",
      "https://smith.langchain.com/o/99caf4db-f19e-57e9-bc93-12af0e3dd026/datasets/290c08a2-d150-4a7b-a243-60db3c6f07b5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must specify reference_key in smith_eval.RunEvalConfig to use evaluator of type qa with dataset with multiple output keys: None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 41\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ({\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: ({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]}),\n\u001b[0;32m     38\u001b[0m     } \u001b[38;5;241m|\u001b[39m chain)\n\u001b[1;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple_llm_testing-3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#KEEP THIS URL https://smith.langchain.com/public/cfa11397-dafa-4936-9548-a984b33f1658/d\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# https://smith.langchain.com/public/17ab53ee-6971-41dd-b8ed-a9b41390aed0/d\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiheu\\LLMTesting\\LLMTesting\\.venv\\Lib\\site-packages\\langsmith\\client.py:3153\u001b[0m, in \u001b[0;36mClient.run_on_dataset\u001b[1;34m(self, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, verbose, tags, input_mapper, revision_id)\u001b[0m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   3149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   3150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe client.run_on_dataset function requires the langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage to run.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstall with pip install langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3152\u001b[0m     )\n\u001b[1;32m-> 3153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3163\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3165\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiheu\\LLMTesting\\LLMTesting\\.venv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:1298\u001b[0m, in \u001b[0;36mrun_on_dataset\u001b[1;34m(client, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, verbose, tags, revision_id, **kwargs)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     warn_deprecated(\n\u001b[0;32m   1291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1292\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following arguments are deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1296\u001b[0m     )\n\u001b[0;32m   1297\u001b[0m client \u001b[38;5;241m=\u001b[39m client \u001b[38;5;129;01mor\u001b[39;00m Client()\n\u001b[1;32m-> 1298\u001b[0m container \u001b[38;5;241m=\u001b[39m \u001b[43m_DatasetRunContainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concurrency_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1311\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1312\u001b[0m         _run_llm_or_chain(\n\u001b[0;32m   1313\u001b[0m             example,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1318\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(container\u001b[38;5;241m.\u001b[39mexamples, container\u001b[38;5;241m.\u001b[39mconfigs)\n\u001b[0;32m   1319\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\jiheu\\LLMTesting\\LLMTesting\\.venv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:1137\u001b[0m, in \u001b[0;36m_DatasetRunContainer.prepare\u001b[1;34m(cls, client, dataset_name, llm_or_chain_factory, project_name, evaluation, tags, input_mapper, concurrency_level, project_metadata, revision_id)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     tags\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1136\u001b[0m wrapped_model \u001b[38;5;241m=\u001b[39m _wrap_in_chain_factory(llm_or_chain_factory)\n\u001b[1;32m-> 1137\u001b[0m run_evaluators \u001b[38;5;241m=\u001b[39m \u001b[43m_setup_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m _validate_example_inputs(examples[\u001b[38;5;241m0\u001b[39m], wrapped_model, input_mapper)\n\u001b[0;32m   1141\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m progress\u001b[38;5;241m.\u001b[39mProgressBarCallback(\u001b[38;5;28mlen\u001b[39m(examples))\n",
      "File \u001b[1;32mc:\\Users\\jiheu\\LLMTesting\\LLMTesting\\.venv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:436\u001b[0m, in \u001b[0;36m_setup_evaluation\u001b[1;34m(llm_or_chain_factory, examples, evaluation, data_type)\u001b[0m\n\u001b[0;32m    434\u001b[0m         run_inputs \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minput_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chain, Chain) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    435\u001b[0m         run_outputs \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39moutput_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chain, Chain) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     run_evaluators \u001b[38;5;241m=\u001b[39m \u001b[43m_load_run_evaluators\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# TODO: Create a default helpfulness evaluator\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     run_evaluators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiheu\\LLMTesting\\LLMTesting\\.venv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:615\u001b[0m, in \u001b[0;36m_load_run_evaluators\u001b[1;34m(config, run_type, data_type, example_outputs, run_inputs, run_outputs)\u001b[0m\n\u001b[0;32m    611\u001b[0m     input_key, prediction_key, reference_key \u001b[38;5;241m=\u001b[39m _get_keys(\n\u001b[0;32m    612\u001b[0m         config, run_inputs, run_outputs, example_outputs\n\u001b[0;32m    613\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eval_config \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mevaluators:\n\u001b[1;32m--> 615\u001b[0m     run_evaluator \u001b[38;5;241m=\u001b[39m \u001b[43m_construct_run_evaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprediction_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m     run_evaluators\u001b[38;5;241m.\u001b[39mappend(run_evaluator)\n\u001b[0;32m    626\u001b[0m custom_evaluators \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mcustom_evaluators \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[1;32mc:\\Users\\jiheu\\LLMTesting\\LLMTesting\\.venv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:542\u001b[0m, in \u001b[0;36m_construct_run_evaluator\u001b[1;34m(eval_config, eval_llm, run_type, data_type, example_outputs, reference_key, input_key, prediction_key)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(evaluator_, StringEvaluator):\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m evaluator_\u001b[38;5;241m.\u001b[39mrequires_reference \u001b[38;5;129;01mand\u001b[39;00m reference_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust specify reference_key in smith_eval.RunEvalConfig to use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m evaluator of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_type_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dataset with multiple output keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_outputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         )\n\u001b[0;32m    547\u001b[0m     run_evaluator \u001b[38;5;241m=\u001b[39m smith_eval\u001b[38;5;241m.\u001b[39mStringRunEvaluatorChain\u001b[38;5;241m.\u001b[39mfrom_run_and_data_type(\n\u001b[0;32m    548\u001b[0m         evaluator_,\n\u001b[0;32m    549\u001b[0m         run_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m         tags\u001b[38;5;241m=\u001b[39m[eval_type_tag],\n\u001b[0;32m    555\u001b[0m     )\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(evaluator_, PairwiseStringEvaluator):\n",
      "\u001b[1;31mValueError\u001b[0m: Must specify reference_key in smith_eval.RunEvalConfig to use evaluator of type qa with dataset with multiple output keys: None."
     ]
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "\n",
    "# eval_config = RunEvalConfig(\n",
    "#     # We will use the chain-of-thought Q&A correctness evaluator\n",
    "#     evaluators=[\"cot_qa\"],\n",
    "# )\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        # RunEvalConfig.Criteria({\"creativity\": \"Is this submission creative, imaginative, or novel?\"}),\n",
    "        # We provide some simple default criteria like \"conciseness\" you can use as well\n",
    "        RunEvalConfig.Criteria(\"conciseness\"),\n",
    "        RunEvalConfig.Criteria(\"relevance\"),\n",
    "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
    "        RunEvalConfig.Criteria(\"coherence\"),\n",
    "        RunEvalConfig.Criteria(\"maliciousness\"),\n",
    "        RunEvalConfig.Criteria(\"helpfulness\"),\n",
    "        RunEvalConfig.Criteria(\"controversiality\"),\n",
    "        RunEvalConfig.Criteria(\"misogyny\"),\n",
    "        RunEvalConfig.Criteria(\"criminality\"),\n",
    "        RunEvalConfig.Criteria(\"insensitivity\"),\n",
    "        RunEvalConfig.Criteria(\"depth\"),\n",
    "        RunEvalConfig.Criteria(\"creativity\"),\n",
    "        RunEvalConfig.Criteria(\"detail\"),\n",
    "    ],\n",
    "    input_key=\"question\"\n",
    ")\n",
    "\n",
    "def construct_chain():\n",
    "    # Add a step to convert the data from the dataset to a form the chain can consume\n",
    "    x = itemgetter(\"question\")\n",
    "    print(x)\n",
    "    return ({\n",
    "        \"question\": lambda x: ({\"question\": x[\"question\"]}),\n",
    "    } | chain)\n",
    "\n",
    "\n",
    "results = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=evaluation_config,\n",
    "    project_name=\"simple_llm_testing-3\",\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "#KEEP THIS URL https://smith.langchain.com/public/cfa11397-dafa-4936-9548-a984b33f1658/d\n",
    "# https://smith.langchain.com/public/17ab53ee-6971-41dd-b8ed-a9b41390aed0/d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
